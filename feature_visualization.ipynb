{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Code basiert auf: https://github.com/rachtibat/zennit-crp/tree/master/tutorials und wählt die 8 Bilder mit der höchsten Relevanz für die ausgewählten Features.\n",
    "\n",
    "Die Features habe ich mit Hilfe des JS-Scripts [getRel.mjs](getRel.mjs) ausgewählt.\n",
    "\n",
    "Dieses Script nutzt die [crp](crp.yml) conda umgebung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.vgg import vgg16_bn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from zennit.canonizers import SequentialMergeBatchNorm\n",
    "from zennit.composites import EpsilonPlusFlat\n",
    "\n",
    "import os\n",
    "\n",
    "# Only tested mps, if it doesnt run on NVIDIA Cuda try to run on CPU \n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "# device = torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "weights = models.VGG16_BN_Weights.IMAGENET1K_V1.DEFAULT\n",
    "model = vgg16_bn(weights=weights).to(device)\n",
    "model.eval()\n",
    "\n",
    "canonizers = [SequentialMergeBatchNorm()]\n",
    "composite = EpsilonPlusFlat(canonizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from crp.concepts import ChannelConcept\n",
    "from crp.helper import get_layer_names\n",
    "from crp.attribution import CondAttribution\n",
    "from crp.visualization import FeatureVisualization\n",
    "from VGG16_ImageNet.download_imagenet import download\n",
    "\n",
    "cc = ChannelConcept()\n",
    "\n",
    "layer_names = get_layer_names(model, [torch.nn.Conv2d, torch.nn.Linear])\n",
    "layer_map = {layer : cc for layer in layer_names}\n",
    "\n",
    "attribution = CondAttribution(model)\n",
    "\n",
    "transform = T.Compose([T.Resize(256), T.CenterCrop(224), T.ToTensor()])\n",
    "preprocessing =  T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "data_path = \"ImageNet_data\"\n",
    "\n",
    "if data_path is None:\n",
    "    data_path = \"ImageNet_data\"\n",
    "    download(data_path)\n",
    "    \n",
    "# apply no normalization here!\n",
    "imagenet_data = torchvision.datasets.ImageNet(data_path, transform=transform, split=\"val\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Tuple, Iterable\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "def plot_grid_save(ref_c: Dict[int, Any], filename: str, cmap_dim=1, cmap=\"bwr\", \n",
    "                   vmin=None, vmax=None, symmetric=True, resize=None, padding=True, figsize=(6, 6)):\n",
    "    keys = list(ref_c.keys())\n",
    "    nrows = len(keys)\n",
    "    value = next(iter(ref_c.values()))\n",
    "\n",
    "    if cmap_dim not in (0, 1):\n",
    "        raise ValueError(\"'cmap_dim' must be 0 or 1.\")\n",
    "\n",
    "    if isinstance(value, Tuple) and isinstance(value[0], Iterable):\n",
    "        nsubrows = len(value)\n",
    "        ncols = len(value[0])\n",
    "    elif isinstance(value, Iterable):\n",
    "        nsubrows = 1\n",
    "        ncols = len(value)\n",
    "    else:\n",
    "        raise ValueError(\"'ref_c' dictionary must contain an iterable of torch.Tensor, np.ndarray or PIL Image or a tuple of thereof.\")\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    outer = gridspec.GridSpec(nrows, 1, wspace=0, hspace=0.2)\n",
    "\n",
    "    for i in range(nrows):\n",
    "        inner = gridspec.GridSpecFromSubplotSpec(nsubrows, ncols, subplot_spec=outer[i], wspace=0, hspace=0.1)\n",
    "\n",
    "        for sr in range(nsubrows):\n",
    "\n",
    "            if nsubrows > 1:\n",
    "                img_list = ref_c[keys[i]][sr]\n",
    "            else:\n",
    "                img_list = ref_c[keys[i]]\n",
    "\n",
    "            for c in range(ncols):\n",
    "                ax = plt.Subplot(fig, inner[sr, c])\n",
    "\n",
    "                if sr == cmap_dim:\n",
    "                    img = imgify(img_list[c], cmap=cmap, vmin=vmin, vmax=vmax,\n",
    "                                 symmetric=symmetric, resize=resize, padding=padding)\n",
    "                else:\n",
    "                    img = imgify(img_list[c], resize=resize, padding=padding)\n",
    "\n",
    "                ax.imshow(img)\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "\n",
    "                if sr == 0 and c == 0:\n",
    "                    ax.set_ylabel(keys[i])\n",
    "\n",
    "                fig.add_subplot(ax)\n",
    "\n",
    "    outer.tight_layout(fig)\n",
    "\n",
    "    fig.savefig(filename, format=\"png\", bbox_inches=\"tight\")\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fv_path = \"VGG16_ImageNet\"\n",
    "fv = FeatureVisualization(attribution, imagenet_data, layer_map, preprocess_fn=preprocessing, path=fv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from crp.image import plot_grid\n",
    "from crp.image import imgify\n",
    "from crp.image import vis_opaque_img\n",
    "\n",
    "rel = [327, 71, 239, 79, 340, 465, 19, 146, 113, 261, 321, 57, 107, 282, 117, 476, 168, 194, 60, 484, 384, 325, 351, 326, 16, 504, 115, 30, 210, 373, 492, 307, 25, 153, 195, 294, 152, 260, 42, 469, 35, 161, 89, 316, 401, 256, 507, 306, 361, 12, 451, 39, 202, 211, 277, 324, 188, 236, 203, 328, 292, 354, 131, 487]\n",
    "\n",
    "destinationFolder = \"results_attr_show\"\n",
    "isExist = os.path.exists(destinationFolder)\n",
    "if not isExist:\n",
    "    os.makedirs(destinationFolder)\n",
    "\n",
    "for r in rel:\n",
    "    ref_c = fv.get_max_reference([r], \"features.40\", \"relevance\", (0, 8), composite=composite, plot_fn=vis_opaque_img)\n",
    "\n",
    "    img = plot_grid_save(ref_c, f\"{destinationFolder}/{r}.png\", cmap=\"bwr\", symmetric=True, figsize=(6, 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c2454eddf0b216369ddcaa6c1a78b4d7c10611a9506483fadb2b8cad3cc9934"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
