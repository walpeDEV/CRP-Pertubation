{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Code berechnet die Heatmaps (LRP) und auch die Heatmaps der relevantesten Features.<br>\n",
    "Außerdem wird von den Pertubierten die Differenz zum Originalbild berechnet und visuell dargestellt (die Differenzberechnung basiert auf den PNG Bildern und nicht auf den rohen Tensor Daten)<br>\n",
    "Außerdem werden noch die relevantesten Features und deren Relevanz in ner txt Datei abgespeichert.\n",
    "\n",
    "Dieses Script nutzt die [crp](crp.yml) conda umgebung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import json\n",
    "import urllib.request\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Code only tested on mps, if it doesnt run on NVIDIA Cuda try to run on CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "# device = torch.device(\"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "imagenet_labels = models.VGG16_Weights.DEFAULT.meta[\"categories\"]\n",
    "\n",
    "\n",
    "weights=models.VGG16_BN_Weights.IMAGENET1K_V1.DEFAULT\n",
    "model = models.vgg16_bn(weights=weights).to(device)\n",
    "model.eval()\n",
    "\n",
    "transform01 = T.Compose([T.Resize(256),T.CenterCrop(224)])\n",
    "transform02 = T.Compose([T.ToTensor(), T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zennit.composites import EpsilonPlusFlat\n",
    "from zennit.canonizers import SequentialMergeBatchNorm\n",
    "from crp.attribution import CondAttribution\n",
    "from crp.concepts import ChannelConcept\n",
    "from crp.helper import get_layer_names\n",
    "\n",
    "from crp.image import imgify\n",
    "\n",
    "composite = EpsilonPlusFlat([SequentialMergeBatchNorm()])\n",
    "attribution = CondAttribution(model, no_param_grad=True)\n",
    "cc = ChannelConcept()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_relevance(concept_ids,rel_values):\n",
    "    concept_ids = concept_ids.cpu().tolist()\n",
    "    rel_values = rel_values.cpu().tolist()\n",
    "\n",
    "    resStr = \"\"\n",
    "    for cid, val in zip(concept_ids, rel_values):\n",
    "        line = f\"{cid}: {val*100:.3f}%\"\n",
    "        # print(line)\n",
    "        resStr+=line+'\\n'\n",
    "    return resStr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(img_src):\n",
    "    image = Image.open(img_src)\n",
    "\n",
    "    image = transform01(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crp(sample):\n",
    "    sample.requires_grad = True\n",
    "    output = model(sample)  \n",
    "    prediction = torch.argmax(output, dim=1).item()\n",
    "    prediction_text = imagenet_labels[prediction]\n",
    "    \n",
    "    # print(f\"Vorhergesagte Klasse: {prediction_text} ({prediction})\")\n",
    "\n",
    "    conditions = [{\"y\": [prediction]}]\n",
    "    attr = attribution(sample, conditions, composite)\n",
    "\n",
    "    totalHeatMap = attr.heatmap\n",
    "    \n",
    "    layer_names = get_layer_names(model, [torch.nn.Conv2d, torch.nn.Linear])\n",
    "\n",
    "    attr = attribution(sample, conditions, composite, record_layer=layer_names)\n",
    "\n",
    "    attr.activations['features.40'].shape, attr.relevances['features.40'].shape\n",
    "\n",
    "    # layer features.40 has 512 channel concepts\n",
    "    rel_c = cc.attribute(attr.relevances['features.40'], abs_norm=True)\n",
    "    rel_c.shape\n",
    "\n",
    "    # the six most relevant concepts and their contribution to final classification in percent\n",
    "    rel_values, concept_ids = torch.topk(rel_c[0], 6)\n",
    "\n",
    "    conditions = [{'features.40': [id], 'y': [prediction]} for id in concept_ids]\n",
    "\n",
    "    attr = attribution(sample, conditions, composite)\n",
    "    return {\n",
    "        'prediction': prediction,\n",
    "        'total': totalHeatMap,\n",
    "        'topk': attr.heatmap,\n",
    "        'rel_values': rel_values,\n",
    "        'concept_ids': concept_ids,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def display(src):\n",
    "    sample = torch.load(src).to(device).detach()\n",
    "    result = crp(sample)\n",
    "    img = result['total'].squeeze().cpu()\n",
    "    return imgify(img, symmetric=True)\n",
    "\n",
    "def show_diff(img1_path, img2_path):\n",
    "    img1 = np.array(Image.open(img1_path).convert(\"RGB\")).astype(np.float32) / 255.0\n",
    "    img2 = np.array(Image.open(img2_path).convert(\"RGB\")).astype(np.float32) / 255.0\n",
    "\n",
    "    diff = np.abs(img1 - img2)\n",
    "\n",
    "    # plt.figure(figsize=(12,4))\n",
    "    # plt.subplot(1,3,1); plt.imshow(img1); plt.title(\"Image 1\"); plt.axis(\"off\")\n",
    "    # plt.subplot(1,3,2); plt.imshow(img2); plt.title(\"Image 2\"); plt.axis(\"off\")\n",
    "    # plt.subplot(1,3,3); plt.imshow(diff / diff.max()); plt.title(\"Diff (normalized)\"); plt.axis(\"off\")\n",
    "    # plt.show()\n",
    "    \n",
    "    return imgify(diff/diff.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveTop(srcImg,desFolder):\n",
    "    sample = torch.load(srcImg).to(device).detach()\n",
    "    res = crp(sample)\n",
    "    rel_values = res['rel_values']\n",
    "    concept_ids = res['concept_ids']\n",
    "    img = imgify(res['topk'], symmetric=True, grid=(1, len(concept_ids)))\n",
    "    img.save(f\"{desFolder}/topk.png\")\n",
    "    info_str = list_relevance(concept_ids,rel_values)\n",
    "    with open(f\"{desFolder}/info.txt\", \"w\") as f:\n",
    "        pred = res['prediction']\n",
    "        f.write(f\"{imagenet_labels[pred]} ({pred})\\n\")\n",
    "        f.write(\"Relevant Concepts:\\n\")\n",
    "        f.write(info_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAll(dirPath):\n",
    "    first = True\n",
    "    saveTop(f\"{dirPath}/original.pt\",dirPath)\n",
    "    for folder in Path(dirPath).iterdir():\n",
    "        if not folder.is_dir():\n",
    "            continue\n",
    "            \n",
    "        img = show_diff(f\"{dirPath}/original.png\", f\"{folder}/perturbed.png\")\n",
    "        img.save(f\"{folder}/diff.png\")\n",
    "        \n",
    "        saveTop(f\"{folder}/perturbed.pt\",folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveAll(dirPath):\n",
    "    for file in Path(dirPath).rglob(\"*.pt\"):\n",
    "        img = display(str(file))\n",
    "        img.save(f\"{file.parents[0]}/{file.stem}_hmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveAll(\"results/beagle3.png\")\n",
    "calcAll(\"results/beagle3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgNames = [\"beagle1.jpg\",\"dog.jpg\",\"eel1.jpg\",\"espresso.jpg\",\"goldfish.jpg\",\"lizard.jpg\",\"plane.png\",\"snail.png\" , \"warplane.jpg\"]\n",
    "\n",
    "for i in imgNames:\n",
    "    result_path = f\"results/{i}\"\n",
    "    print(result_path)\n",
    "    saveAll(result_path)\n",
    "    calcAll(result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "# This audio-file is not with in the repo\n",
    "IPython.display.Audio(\"audio.mp3\", autoplay=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c2454eddf0b216369ddcaa6c1a78b4d7c10611a9506483fadb2b8cad3cc9934"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
